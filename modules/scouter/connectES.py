from elasticsearch import Elasticsearch
import sys

def keywordGetter(date):
    '''
    Keyword getter method to request keyword set contains related news article 
    to SCOUTER system
    '''
    es = Elasticsearch("155.230.34.145:9200")

    # Get the cluster corresponding to the entered date.
    query = {
        'query':{
            'match':{
                'postingDate': date
            }
        }
    }
    res = es.search(index='keyword_news', body=query)
    
    if res['hits']['total']['value'] == 0:
        return False
    elif res['hits']['hits'][0]['_source']['flag'] == False:
        return False
    
    keyword_cluster = res['hits']['hits'][0]['_source']
    
    return keyword_cluster

class ScouterHandler():
    '''
    Helpper class for client to communicate with Scouter server
    Args:
        addr (string): scouter server address
        size (string): scroll size that client want to get at once
    '''

    def __init__(self, addr='155.230.34.145:9200', size=1000):
        self.addr = addr
        self.size = size

    def delete(self, query_body, data_type):
        es_client = Elasticsearch(self.addr, timeout=30, max_retries=10, retry_on_timeout=True)
        es_client.delete_by_query(index=data_type,
                                  doc_type='_doc',
                                  body=query_body)

    def search(self, query_body, data_type, max_num_doc=sys.maxsize, preprocess_fnc=None, trim_lower=True):
        '''
        Request the data searched by the query on Scouter server
        Args:
            query_body (dict): query to search generated by class method
            data_type (string): newspaper or graphs
            preprocess_fnc (function): preprocess function to refine responses
        Returns:
            doc_info_list (list): list of searched data that is refined by preprocess_fnc
        '''

        # Qassert data_type == 'newspaper' or data_type == 'graphs'

        doc_info_list = []
        if preprocess_fnc == None:
            # 함수 포인터를 넘겨줘서 얘가 이제 함수처럼 쓰임
            # 이제 preprocess_fnc로 사용한다.
            # preprocess_fnc = self._default_preprocess_fnc
            # 우리는 여기서 내걸 쓴다!!
            preprocess_fnc = self._new_preprocess_fnc

        cummulated_num_docs = 0

        es_client = Elasticsearch(self.addr, timeout=30, max_retries=10, retry_on_timeout=True)
        response = es_client.search(index=data_type,
                                    #doc_type='_doc',
                                    scroll='10m',
                                    size=self.size if self.size < max_num_doc else max_num_doc,
                                    body=query_body)
        doc_info_list += preprocess_fnc(response)

        # Get metadata
        scroll_id = response['_scroll_id']
        num_scroll = 1
        num_docs = len(response['hits']['hits'])
        cummulated_num_docs += num_docs
        print("Scroll idx : {} ({} docs)".format(num_scroll, num_docs))

        # Scrolling
        # 스크롤 1000개씩 처리한다.
        while num_docs > 0:
            #이주영 실험용
            '''if len(doc_info_list)>=100000:
                break'''
                #여기까지
            if cummulated_num_docs >= max_num_doc:
                break

            response = es_client.scroll(scroll_id=scroll_id, scroll='10m')
            doc_info_list += preprocess_fnc(response)

            num_scroll += 1
            scroll_id = response['_scroll_id']
            num_docs = len(response['hits']['hits'])
            cummulated_num_docs += num_docs
            if num_docs != 0:
                print("Scroll idx : {} ({} docs)".format(num_scroll, num_docs))

        # 70%만 가져오는 데 중복성을 검사하려면 전부 가져와야 한다
        # 아마 시스템에서 한계가 있는 것 같은데 그건 그때가서 확인
        '''
        if trim_lower and cummulated_num_docs < max_num_doc:
            print("Trim lower 70% of docs ({} docs are cut)".format(int(len(doc_info_list)*0.3)))
            doc_info_list = doc_info_list[:int(len(doc_info_list)*0.7)]
        '''

        print("Total retrieved Doc # : ", len(doc_info_list))
        #print(doc_info_list[0])
        print()

        return doc_info_list

    @classmethod
    def make_keyword_query_body(self, keyword, filters=None):
        '''
        Make a query body with keyword string
        Args:
            keyword (dict): additional matching condition
            filters (list): list of fields to be asked
        Returns:
            query_body (dict): query body to be used on elasticsearch
        '''

        print("Query : {}".format(keyword))
        keyword_list = keyword.split(' ')

        # make a conjugated filter for elasticsearch
        # 여기 doc.으로 수정해야 돌아감★★★★★★★★★★★★★★★★★★★
        and_query = [{"match": {"extContent": word}} for word in keyword_list]  # extContent: 본문
        query_body = {'query': {"bool": {"must": and_query}}}

        if filters is not None:
            query_body['_source'] = filters

        return query_body

    @classmethod
    def make_category_query_body(self, keyword, filters=None):

        print("Query : {}".format(keyword))
        keyword_list = keyword.split(' ')

        # make a conjugated filter for elasticsearch
        # 여기 doc.으로 수정해야 돌아감★★★★★★★★★★★★★★★★★★★
        and_query = [{"match": {"category": word}} for word in keyword_list]  # extContent: 본문
        query_body = {'query': {"bool": {"must": and_query}}}
        if filters is not None:
            query_body['_source'] = filters

        return query_body

    @classmethod
    def make_all_query_body(self, keyword, filters=None):

        print("Query : {}".format(keyword))
        keyword_list = keyword.split(' ')

        # make a conjugated filter for elasticsearch
        # 여기 doc.으로 수정해야 돌아감★★★★★★★★★★★★★★★★★★★
        and_query = [{"regexp": {"extContent": word}} for word in keyword_list]  # extContent: 본문
        query_body = {'query': {"bool": {"must": and_query}}}

        # 이주영 실험용 query_body
        cate = ['정치', '경제', '사회', '생활문화', '세계']
        query_body2 = {"query": {"bool": {"must": and_query, "filter": {"term": {"category": cate[4]}}}}}

        if filters is not None:
            query_body['_source'] = filters

        return query_body

    #키워드 예측용 날짜 쿼리
    @classmethod
    def make_date_query_body(self, postingdate, filters=None):
        print("Query: {}".format(postingdate))
        query_body = {"query": { "bool":{"filter": {"range": {"postingDate": {"gte": postingdate,"lte": postingdate}}}}}}

        if filters is not None:
            query_body['_source'] = filters

        return query_body

    @classmethod
    def make_title_query_body(self, keyword, filters=None):
        print("Query : {}".format(keyword))

        # make a conjugated filter for elasticsearch
        # 여기 doc.으로 수정해야 돌아감★★★★★★★★★★★★★★★★★★★
        and_query = {"match": {"newsTitle": keyword}}  # extContent: 본문
        query_body = {'query': and_query}

        if filters is not None:
            query_body['_source'] = filters

        return query_body

    @classmethod
    def make_doc_id_query_body(self, doc_id_list, filters=None):
        '''
        Make a query body with keyword string
        Args:
            opts (dict): additional matching condition
            filters (list): list of fields to be asked
        Returns:
            query_body (dict): query body to be used on elasticsearch
        '''

        # make a conjugated filter for elasticsearch
        # TODO: 필요한 정보만 가져올 수 있게 되면 filter도 같이 달아줘야 함
        or_query = {"match": {"news_id": ' '.join(map(str, doc_id_list))}}
        query_body = {'query': or_query}
        if filters is not None:
            query_body['_source'] = filters

        return query_body

    def _default_preprocess_fnc(self, response):
        '''
        Default preprocess function that gather relevance score and all fields of data
        '''

        doc_info_list = []
        for data in response['hits']['hits']:
            # data의 _source정보를 doc_info에 저장한다 => dictionary라서 key:value로 저장!!
            doc_info = data['_source']
            # doc_info의 dic에 새로운 'rel_score'에 '_score'값을 추가로 저장한다.
            doc_info['rel_score'] = data['_score']
            # 결국엔 doc_info_list에 값이 저장되어 있다
            doc_info_list.append(doc_info)
        return doc_info_list

    def _new_preprocess_fnc(self, response):
        doc_info_list = []
        for data in response['hits']['hits']:
            # _id와 news_id가 같다고 가정한다
            doc_info = data['_source']
            doc_info_list.append(doc_info)

        return doc_info_list
    
